% * Preamble
% ** Document class
\documentclass[12pt,a4paper]{amsart}
\setlength{\textwidth}{\paperwidth}
\addtolength{\textwidth}{-2in}
\calclayout
% ** Encoding information
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
% \usepackage[UTF8]{ctex} 
% You need XeTeX to build ctex based document 
% ** packages about math
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{mathrsfs}
\usepackage{stackrel}
\usepackage[backref]{hyperref}
\usepackage{comment}
% ** theorem environment definition
\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollaray}
\newtheorem{conj}[thm]{Conjecture}
\theoremstyle{definition}
\newtheorem{defi}[thm]{Definition}
\newtheorem{exa}[thm]{Example}
\newtheorem{asp}{Assumption}
\theoremstyle{remark}
\newtheorem*{exe}{Exercise}
\newtheorem{rem}[thm]{Remark}
\numberwithin{equation}{section}
\allowdisplaybreaks
% * Document
% ** BEGIN
\begin{document}
% ** Basic Info
\title
[short title]
{long title}
\author
[Z. Sun]
{Zhenyao Sun}
\address
{Zhenyao Sun\\
School of Mathematical Sciences\\
Peking University\\
Beijing, P. R. China, 100871}
\email{zhenyao.sun@pku.edu.cn}
\begin{abstract}
  TBD
\end{abstract}
\maketitle
% ** Section: HW for 6.5
% *** 6.5.1.
\begin{exe}[6.5.1]
To show that the convergence in (a) of Theorem 6.4.1. may occur arbitrarily slowly, let $X_{m,m+k}=f(k)\geq 0$, where $f(k)/k$ is decreasing, and check that $X_{m,m+k}$ is subaddditive.
\end{exe}
\begin{proof}
Verify (i): 
\begin{align}
X_{0,m}+X_{m,n} 
&= f(m) + f(n-m)
= m \frac{f(m)}{m} + (n-m)  \frac{f(n-m)}{n-m} 
\\&\geq m \frac{f(n)}{n} + (n-m) \frac{f(n)}{n}
= f(n) 
= X_{0,n}.
\end{align}

Verify (ii): For each $k$, $(X_{nk,(n+1)k})_{n\geq 1}= (f(k))_{n\geq 1}$ is obviously a stationary sequence.

Verify (iii): The distribution of $(X_{m,m+k})_{k\geq 1} = (f(k))_{k\geq 1}$ obviously does not depend on $m$.

Verify (iv): Obviously $EX_{0,1}^+=f(1)<\infty$. 
Denote by $\gamma_0 := \lim_{k\to \infty} f(k)/k \geq 0$, then we do have $EX_{0,n}= f(n)\geq \gamma_0 n$ and $\gamma_0 > -\infty$. 
\end{proof}
% *** 6.5.2.
\begin{exe}[6.5.2.]
Consider the longest common subsequence problem, Example 6.4.4. when $X_1,X_2,\dots$ and $Y_1,Y_2,\dots$ are i.i.d. and take the values $0$ and $1$ with probability $1/2$ each.
(a) Compute $EL_1$ and $EL_2/2$ to get lower bounds on $\gamma$.
(b) Show $\gamma < 1$ by computing the expected number of $i$ and $j$ sequence of length $K=an$ with the desired property. 
\end{exe}
\begin{proof}
(a) Since $L_{0,1} = [0,1] \cap \mathbb Z$, we have
\begin{align}
E L_{0,1} 
= P(L_{0,1} = 1) 
= P(X_1 = Y_1)
= 1/2.
\end{align}
Similarly, since $L_{0,2} = [0,2]\cap \mathbb Z$, we have
\begin{align}
E L_{0,2}
& = P(L_{0,2} = 1) + 2P(L_{0,2} = 2)
= \left( 1- P(L_{0,2} = 0) - P(L_{0,2} = 2) \right) + 2 P(L_{0,2} = 2)
\\&= 1 + P(L_{0,2} = 2) - P(L_{0,2} = 0)
= 1+ P(X_1=Y_1, X_2= Y_2) - P(X_1 = X_2 \neq Y_1 = Y_2)
\\& = 1 + 1/4 - 1/8 
= 9/8
\end{align} 
So we already know that 
\begin{align}
\gamma = \sup_{m\geq 1} E(L_{0,m})/m \geq 9/16.
\end{align}

(b) For each $k,n \in \mathbb N$ with $k\leq n$, denote by $\mathcal I_{n,k}:= \{ (i_1,\dots, i_k): 1< i_1<\dots < i_k \leq n \}$ the collection of all increasing multi-index with length $k$ in the index space $\{1,2,\dots,n\}$.
For each $I = (i_1,\dots, i_k) \in \mathcal I_{n,k}$, denote by
$X_I:= (X_{i_1},X_{i_2},\dots, X_{i_k})$ the $I$-subsequence of the process $(X_k)_{k\in \mathbb N}$.
Similarly, we can define $Y_I$ for each $I\in \mathcal I_{n,k}$. 
Note that there exists a constant $C>0$ such that for each integers $0 < k < n$, we have
\begin{align}
P\left( L_{0,n} \geq k \right)
&= P(\exists I,J\in \mathcal I_{n,k} ~\text{s.t.}~ X_I = Y_J)
\leq \sum_{I,J\in \mathcal I_{n,k}} P(X_I =Y_J)
\\&= \# \{I,J\in \mathcal I_{n,k}\}\cdot 2^{-k}
=\left(\frac{n!}{k!(n-k)!}\right)^2  2^{-k}
\\&\leq C \left( \frac{n^{n+1/2}e^{-n}}{k^{k+1/2}e^{-k} (n-k)^{n-k+1/2} e^{-{n-k}}} 2^{-k}\right)^2,\quad \text{by Stirling's formula}
\\& \overset{a:=n/k}{=} C \left( \frac{n^{n+1/2}}{(an)^{an+1/2} ((1-a)n)^{(1-a)n+1/2}} 2^{-na} \right)^2 
%! \\& = C \left( \frac{1}{a^{an+1/2} n^{1/2} (1-a)^{(1-a)n+1/2}} 2^{-na} \right)^2 
%! \\& = C n^{-1}\exp \left( -2 (an+1/2)\ln a - 2((1-a)n +1/2)\ln (1-a) - 2na\ln 2 \right)
%! \\& = C n^{-1}\exp \left( -2 n a\ln a  -\ln a - 2(1-a)n \ln(1-a) -\ln (1-a) - 2na\ln 2 \right)
%! \\& = C n^{-1}\exp \left( -2 n (a\ln a+(1-a)\ln (1-a) + a \ln 2)  -\ln a(1-a) \right)
\\& = C \frac{1}{a(1-a)n}\exp \left( -2 n\ln \left ( a^a (1-a)^{1-a} 2^a\right)  \right).
\end{align}
Denote by $g(a) = a^a(1-a)^{1-a}2^a$ for each $a \in (0,1)$, then it holds that $g(a) \xrightarrow[a\uparrow 1]{} 2$, which says that there exists an $0<a_0 < 1$ such that $g(a_0) > 1$. 
Now taking $k_n = \lfloor a_0 n \rfloor$, according to $a_n:=k_n/n \to a_0$, we have
\begin{align}
\sum_{n \in \mathbb N} P\left( \frac{L_{0,n}}{n} \geq a_0 \right)
\leq \sum_{n\in \mathbb N} P\left(L_{0,n} \geq k_n \right)
\leq  C \sum_{n\in \mathbb N} \frac{1}{a_n(1-a_n)n}\exp \left( -2n \ln g(a_n) \right) < \infty.
\end{align}
Therefore, B-C lemma says that almost surely
\begin{align}
\gamma = \lim_{n\to \infty} \frac{L_{0,n}}{n} \leq a_0 < 1.
\end{align}
\end{proof}
% *** 6.5.3
\begin{exe}[6.5.3.]
Given a rate one Poisson process in $[0,\infty)\times [0,\infty)$, let $X_1,Y_1$ be the point that minimizes $x+y$.
Let $(X_2,Y_2)$ be the point in $[X_1,\infty) \times [Y_1,\infty)$ that minimizes $x+y$, and so on.
Use this construction to show that in Example 6.5.2. $\gamma \geq (8/\pi)^{1/2} > 1.59$. 
\end{exe} 
\begin{proof}
The definition of rate one Poisson point process $N$ is given by Example 3.7.7.. 
More precisely, $N(\omega,A)$ is a random measure on $[0,\infty)^2$ such that
\begin{itemize}
\item
For each $w\in \Omega$, $N(w,\cdot)$ is a $\mathbb N\cap \{\infty\}$-valued measure on $[0,\infty)^2$.
\item
For each Borel subset $A\subset [0,\infty)^2$, $N(\cdot, A)$ is a Poisson distributed random variable with mean $\mu(A)$, the Lebesgue measure of $A$.
\end{itemize}
Note that $N$ is a (random) atomic measure, therefore, it is valid to talk about the points of $N$.
According to its definition, $(X_1,Y_1)$ is a point of the atomic measure $N$ in $[0,\infty)^2$ which minimizes $x+y$. 
And $(X_2,Y_2)$ is the point in $[X_1,\infty) \times [Y_1,\infty)$ which minimizes $x+y$. 

We admit the following fact:
$\left(  (X_{k+1}, Y_{k+1}) - (X_k, Y_k)\right)_{k \in \mathbb N}$ are i.i.d. random variables with the same distribution of $(X_1,Y_1)$.
This fact is crucial. Its proof relies on the strong Markov property of the Poisson point processes. Here we omit the proof.

Note that for each $t\geq 0$, we have
\begin{align}
&P(X_1+Y_1 > t) 
= P \left( N\left\{(x,y): x\geq 0, y\geq 0, x+y\leq t \right\} = 0 \right)
\\& = e^{- \int_{(x,y): x\geq 0, y\geq 0, x+y \leq t} \mu(dx,dy)}
= e^{-\frac{t^2}{2}}.
\end{align}
Therefore, 
\begin{align}
E[X_1+Y_1] 
= \int_0^\infty P(X_1 + Y_1 > t) dt 
= \sqrt{\frac{\pi}{2}},
\end{align}
which, thanks to the symmetry, says that $EX_1 = EY_1 = \sqrt{\frac{\pi}{8}}$.
Now, from Law of large numbers, we have almost surely
\begin{align}
\frac{Z_n}{n}:= \frac{\max(X_n,Y_n)}{n} \xrightarrow[n \to \infty]{} \sqrt{\frac{\pi}{8}}. 
\end{align}
On the other hand, denoted by $L_{0,s}$ length of the longest increasing path lying in the square $R_{0,n}$ with vertices $(0,0)$, $(0,s)$, $(s,0)$ and $(s,s)$.
(Here the length of a path is simply the number of the 'Poisson' points on that path. So the length of a path is always an integer number.)  
It is now obvious that $(X_1,Y_2),\dots, (X_n,Y_n)$ forms an increasing path in the square $R_{0, Z_n}$. 
Therefore, we have $L_{0,Z_n} \geq n$.
Using the result in Example 6.5.2., we have
\begin{align}
\gamma 
= \lim_{n\to \infty} \frac{L_{0, Z_n}}{ Z_n}  
\geq \lim_{n\to \infty} \frac{n}{ Z_n}
= \sqrt{\frac{8}{\pi}}.
\end{align}
\end{proof}
% *** 6.5.4.
\begin{exe}[6.5.4.]
Let $\pi_n$ be a random permutation of $\{1,\dots,n\}$ and let $J_k^n$ be the number of subsets of $\{1,\dots,n\}$ of size $k$ so that the associated $\pi_n(j)$ form an increasing subsequence.
Compute $EJ_k^n$ and take $k\sim \alpha n^{1/2}$ to conclude that in Example 6.5.2. $\gamma \leq e$.
\end{exe}
\begin{proof}
For each $k,n \in \mathbb N$ with $k\leq n$, denote by $\mathcal H_{n,k}$ the collection of all subset of $\{1,\dots, n\}$ with length $k$. For each $h \in \mathcal H_{n,k}$, we write $h = \{h_i: i=1,\dots, k\}$ such that $0< h_1< \dots < h_k\leq n$. 
There exists a constant $C>0$ such that for each $0<k< n$, we have
\begin{align}
&E J_k^n 
= \sum_{h \in \mathcal H_{n,k}} P\left( \left(\pi_n(h_i)\right)_{i=1}^k~\text{is increasing} \right)
= \# \mathcal H_{n,k} \cdot \frac{ C_n^k \cdot (n-k)!}{n!}
\\&= \frac{n!}{k!(n-k)!k!}
\leq C
\end{align}
\end{proof}
% *** 6.5.5.
\begin{exe}[6.5.5.]
Let $\phi(\theta) = E \exp(-\theta t_i)$ and
\begin{align}
Y_n =\left( \mu \phi(\theta) \right)^{-n} \sum_{i=1}^{Z_n} \exp\left( -\theta T_n(i) \right)
\end{align}
where the sum is over individuals in generation $n$ and $T_n(i)$ is the $i$th person's birth time.
Show that $Y_n$ is a nonnegative martingale and use this to conclude that if $\exp(-\theta a)/\mu \phi(\theta) > 1$, then $P(X_{0,n}\leq an)\to 0$. 
A little thought reveals that this bound is the same as the answer in the answer in the last exercise.
\end{exe}
% ** END
\end{document}