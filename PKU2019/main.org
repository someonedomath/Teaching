#+Title: TA's notes for the stochastic processes class at Peking University, 2019
#+Author: Zhenyao Sun
#+OPTIONS: num:3
#+OPTIONS: H:10
#+Latex_header:\usepackage{mathtools} \mathtoolsset{showonlyrefs}


* Basic Information
***** I am the teaching assistant in the class Advanced in Stochastic processes at Peking University, spring 2019.
***** Here is a link of the Book we use https://services.math.duke.edu/~rtd/PTE/PTE5_011119.pdf.
***** For more information about this class see http://www.math.pku.edu.cn/teachers/dayue/Homepage/instruction.html.
* Solutions to the Selected Exercises 
** Solutions to Exercises 6.3.x and 6.2.x
*** Exercise 6.2.1.
    Show tha if $X\in L^p$ with $p>1$ then the convergence in Theorem 6.2.1 occurs in $L^p$.
*** Proof:
**** Take an arbitrary $M > 0$.
     Let $X'_M := X \mathbf 1_{|X|\leq M}$ and $X''_M:= X \mathbf 1_{|X| > M}$.
**** We claim that
     \[
     \limsup_{n\to \infty}\Big\|\frac{1}{n} \sum_{k=0}^{n-1} X\circ \varphi^k - E[X|\mathcal I]\Big\|_p
     \leq 2\|X_M''\|_p.
     \]
***** In fact, on one hand we have
      \[
      \frac{1}{n} \sum_{k= 0}^{n-1}X'_M \circ \varphi^k 
      \xrightarrow[n\to \infty]{a.s.\& L^p} E[X'_M|\mathcal I],
      \]
      where the almost sure convergence is due to Ergodic theorem, and the $L^p$ convergence is then followed by bounded convergence theorem. 
***** On the other hand, we have
      \begin{align*}
      \Big\|\frac{1}{n}\sum_{k=1}^{n-1} X_M'' \circ \varphi^k - E[X''_M|\mathcal I] \Big\|_p
      &\leq \frac{1}{n}\sum_{k=1}^{n-1}\| X_M'' \circ \varphi^k \|_p +\| E[X''_M|\mathcal I] \|_p
      \\& \leq 2\|X_M''\|_p,
      \quad n\geq 0.
      \end{align*}
**** Now, since $M$ is arbitrary and that $\|X''_M\|_p \to 0$ as $M \to \infty$, we get the desierd result.
$\Box$

*** Exercise 6.2.2 
    (1) Show that if $g_n(w) \to g(w)$ a.s. and $E(\sup_k| g_k|)< \infty$, then \[ \lim_{n\to \infty}\frac{1}{n} \sum_{m=0}^{n-1} g_m(\varphi^m w) = E(g|\mathcal I) \quad a.s. \]
*** Proof:
**** We claim that
\[
\limsup_{n\to \infty} \frac{1}{n} \sum_{m=0}^{n-1} g_m \circ \varphi
\leq E[g|\mathcal I]
\quad \text{a.s.}.
\]
***** In fact, taking an arbitrary $M > 0$, we can define a almost sure finite random variable $h_M:= \sup_{m\geq M}|g_m - g|$ using the condition $E(\sup_k| g_k|)< \infty$.
***** Then we have by ergodic theorem that
\begin{align*}
\frac{1}{n}\sum_{m=0}^{n-1} g_m \circ \varphi^m
&\leq \frac{1}{n}\sum_{m=0}^{M - 1}(g + h_0) \circ \varphi^m + \frac{1}{n}\sum_{m=M}^{n-1}( g + h_M) \circ \varphi^m
\\& \xrightarrow[n\to \infty]{} E[g+h_M|\mathcal I]
\quad a.s..
\end{align*}
***** According to the fact that $|h_M| \leq g+ \sup_{k}|g_k|$ and $h_M \to 0$ as $M \to \infty$, we have $E[g+h_M|\mathcal I] \to E[g|\mathcal I]$ a.s. due to the dominated convergence theorem.
***** Therefore, the claim is true.
**** Applying this claim to $(-g_n)_{n=1,\dots}$, we get that
\[
\liminf_{n\to \infty} \frac{1}{n} \sum_{m=0}^{n-1} g_m \circ \varphi
\geq E[g|\mathcal I]
\quad \text{a.s.}.
\]
$\Box$
*** Exercise 6.2.3
    Let $X_j = X\circ \varphi^j$, $S_k = X_0 + \dots + X_{k-1}$, $A_k = S_k/k$ and $D_k = \max(A_1,\dots, A_k)$. 
    Show that if $\alpha > 0$ then
    \[
    P(D_k > \alpha) \leq \alpha^{-1} E|X|.
    \]
*** Proof:
**** Define $X'_j = X_j - \alpha$,  $S'_k = X'_0 + \dots + X_{k-1}$, $A'_k = S'_k/k$ and $D'_k = \max(A'_1,\dots, A'_k)$.
**** Then, it is easy to see that $S'_k = S_k -\alpha k$, $A'_k = A_k - \alpha$ and $D'_k = D_k -\alpha$.
**** Lemma 6.2.2. says that $E[X'; D'_k>0]\geq 0$.
**** Therefore $E|X| \geq E[X; D_k > \alpha] \geq \alpha P(D_k > \alpha)$ as desired.
$\Box$
*** Exercise 6.3.1 
    Let $g_n = P(S_1 \neq 0, \dots, S_n \neq 0)$ for $n \geq 1$ and $g_0 = 1$. 
    Show that
\[
    E R_n = \sum_{m = 1}^n g_{m-1}.
\]
    (Where $S_n$ and $R_n$ is the same as Theorem 6.3.1.)
*** Proof:
**** Note taht
\[
R_n 
= 1+1_{S_{n-1} \not\in \{S_n\}} + 1_{S_{n-2}\not\in\{ S_{n-1}, S_{n} \}} + \dots + 1_{S_1 \not\in \{S_{2},\dots,S_n\} }.
\]
**** Therefore,
\begin{align*}
ER_n &= 1+\sum_{m=1}^{n-1} P(S_m \not \in \{S_{m+1},\dots, S_n\})
\\&= 1+\sum_{m=1}^{n-1} P(S_{m+1}-S_m \neq 0,\dots,S_n - S_m \neq 0\})
\\&= 1+\sum_{m=1}^{n-1} P(S_1 \neq 0,\dots, S_{n-m} \neq 0\})
\\&= \sum_{m=1}^n g_{m-1}.
\end{align*}
$\Box$
*** Exercise 6.3.2 
    Under the setting of Theorem 6.3.2. Show that if we assume $P(X_i > 1) = 0, E X_i > 0$, and the sequence $X_i$ is ergodic, then $P(A) = EX_i$.
*** Proof:
**** It is elementary analysis that if $s_n /n \to c > 0$, then we must have
\[
n^{-1} \max_{1\leq k\leq n} s_k \to c
\]
and
\[
\inf_{k = 1,\dots} s_k > -\infty.
\]
**** Ergodic theorem syas that
\[
\frac{S_n}{n} \to E X_i > 0,\quad a.s.
\]
so we must have
\[
n^{-1} \max_{1\leq k \leq n} S_k \to EX_i,\quad a.s.
\]
and
\[
M:= \inf_{k=1,\dots} S_k > -\infty, \quad a.s.
\]
**** Note, from the condition $P(X_i > 1) = 0$, we have
\[
\max_{1\leq k \leq n} S_k \leq R_n \leq \max_{1\leq k \leq n} S_k - m,
\]
which now implies that
\[
\frac{R_n}{n} \to EX_i.
\]
**** However, from Theorem 6.3.1. we already know that $n^{-1} R_n \to P(A)$.
**** Therefore, we must have $P(A) = EX_i$.
$\Box$
*** Exercise 6.3.3
    Show that if $P(X_n \in A~\text{at least once}) = 1$ and $A\cap B = \emptyset$ then
    \[
    E\Big( \sum_{1\leq m \leq T_1} 1_{X_m \in B}\Big| X_0 \in A\Big)
    = \frac{P(X_0 \in B)}{P(X_0 \in A)}.
    \]
*** Proof:
**** We can find a two-side stationary process which has the same finite demisional distribution same as $(X_n)_{n\in \mathbb N}$.
     With some abuse of notations, we denote such two-side stationary process as $(X_n)_{n\in \mathbb Z}$.
**** Now, we can verify that
\begin{align*}
     &P(X_0 \in A)E\left[ \sum_{m=1}^{T_1} \mathbf 1_{X_m \in B} \middle| X_0 \in A \right]
     = E\left[ \sum_{t\in \mathbb N}\sum_{m=1}^t \mathbf 1_{X_m \in B, T_1 = t} ; X_0 \in A \right]
     \\&= \sum_{m=1}^\infty P\left( X_m \in B, T_1 \geq m ; X_0 \in A \right)
     = \sum_{m=1}^\infty P\left( X_0\in A, X_1\not\in A,\dots,X_{m-1}\not\in A, X_m \in B \right)
     \\&= \sum_{m=1}^\infty P\left( X_{-m}\in A, X_{-m+1}\not\in A,\dots,X_{-1}\not\in A, X_0 \in B \right)
     = P(X_0 \in B).
\end{align*}
$\Box$
*** Exercise 6.3.4
    Consider the special case in which $X_n \in \{0,1\}$, and let $\bar P = P(\cdot | X_0 = 1)$.
    Here $A= {1}$ and so $T_1 = \inf \{m > 0: X_m = 1\}$.
    Show $P(T_1 = n)= \bar P(T_1 \geq n)/ \bar ET_1$.
*** Proof:
**** From Theorem 6.3.3. we know that $P(X_0 = 1)\bar E T_1 = 1$.
**** Therefore
\begin{align}
\frac{\bar P(T_1\geq n)}{\bar E T_1}
= P(T_1 \geq n|X_0 = 1)P(X_0 = 1)
= P(T_1 \geq n, X_0 = 1).
\end{align}
**** On the other hand, with some abuse of natations, assuming that $(X_n)_{n\in \mathbb Z}$ is a two-sided stationary sequence, we have
\begin{align}
    P(T_1 = n)
&=
    \sum_{m=0}^\infty
        P(X_{-m} = 1, X_{-m+1} = 0,\dots, X_{n-1} = 0, X_n = 1)
\\&= 
    \sum_{m=0}^\infty 
        P( X_0= 1, X_1= 0, \dots, X_{m+n-1}= 0, X_{m+n}= 1)
\\&=
    P(X_0 = 1, T_1 \geq n)
.
\end{align}
$\Box$
